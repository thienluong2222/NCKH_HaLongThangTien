import pandas as pd
import os
from pathlib import Path
import cv2
import numpy as np
from collections import Counter, defaultdict
import matplotlib.pyplot as plt
from ultralytics import YOLO
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser
from langchain_core.output_parsers import StrOutputParser
from constraintsDB import CONSTRAINTS_DB, SUBCLASS_TO_FESTIVAL
import math
from dotenv import load_dotenv
import json

load_dotenv()
API_KEY = os.getenv("GEMINI_API_KEY")

# ==========================================
# PH·∫¶N 1: YOLO PIPELINE 
# ==========================================

class YOLOCSVPipeline:
    def __init__(self, model_path, csv_path):
        """
        Pipeline ƒë·ªÉ detect object b·∫±ng YOLO v√† map v·ªõi CSV

        Args:
            model_path: ƒê∆∞·ªùng d·∫´n ƒë·∫øn model YOLO
            csv_path: ƒê∆∞·ªùng d·∫´n ƒë·∫øn file CSV mapping
        """
        self.model = YOLO(model_path)
        self.mapping_df = pd.read_csv(csv_path)

        # Chu·∫©n h√≥a t√™n c·ªôt
        self.mapping_df.columns = self.mapping_df.columns.str.strip()

        print(f"‚úÖ ƒê√£ load model: {model_path}")
        print(f"‚úÖ ƒê√£ load CSV: {csv_path}")
        print(f"üìä S·ªë d√≤ng trong CSV: {len(self.mapping_df)}")
        print(f"üìã C√°c c·ªôt: {list(self.mapping_df.columns)}")

    def predict_and_map(self, image_path, confidence_threshold=0.5, show_image=True):
        """
        Detect object v√† map v·ªõi CSV
        """
        # 1. YOLO Predict
        results = self.model.predict(image_path, verbose=False)

        detected_items = []  # L∆∞u c·∫£ class_name V√Ä confidence
        matched_results = []

        for result in results:
            if show_image:
                result.show()
            boxes = result.boxes

            if boxes is not None:
                for box in boxes:
                    confidence = float(box.conf)

                    # Ch·ªâ l·∫•y detection c√≥ confidence >= threshold
                    if confidence >= confidence_threshold:
                        class_id = int(box.cls)
                        class_name = result.names[class_id]

                        # ‚úÖ L∆∞u C·∫¢ class_name V√Ä confidence t∆∞∆°ng ·ª©ng
                        detected_items.append({
                            'class_name': class_name,
                            'confidence': confidence
                        })

        # 2. Map v·ªõi CSV
        for item in detected_items:
            detected_class = item['class_name']
            conf = item['confidence']  # ‚úÖ L·∫•y confidence t∆∞∆°ng ·ª©ng

            # T√¨m trong CSV (case-insensitive)
            matches = self.mapping_df[
                self.mapping_df['SubClass'].str.lower() == detected_class.lower()
            ]

            if not matches.empty:
                for _, row in matches.iterrows():
                    matched_results.append({
                        'detected_subclass': detected_class,
                        'mapped_subclass': row['SubClass'],
                        'class': row['Class'],
                        'text': row['Text'],
                        'confidence': conf  # ‚úÖ D√πng confidence ƒë√∫ng
                    })
            else:
                # Kh√¥ng t√¨m th·∫•y mapping
                matched_results.append({
                    'detected_subclass': detected_class,
                    'mapped_subclass': None,
                    'class': None,
                    'text': None,
                    'confidence': conf  # ‚úÖ D√πng confidence ƒë√∫ng
                })

        return matched_results

    def process_single_image(self, image_path, show_unmapped=False):
        """
        X·ª≠ l√Ω 1 ·∫£nh v√† hi·ªÉn th·ªã k·∫øt qu·∫£
        """
        print(f"\n{'='*60}")
        print(f"üñºÔ∏è  ƒêang x·ª≠ l√Ω: {os.path.basename(image_path)}")
        print(f"{'='*60}")

        results = self.predict_and_map(image_path)

        if not results:
            print("‚ùå Kh√¥ng ph√°t hi·ªán object n√†o!")
            return None

        print(f"\n‚úÖ Ph√°t hi·ªán {len(results)} object(s):\n")

        for i, result in enumerate(results, 1):
            if result['mapped_subclass'] is not None:
                print(f"{i}. üéØ Detected: {result['detected_subclass']}")
                print(f"   ‚îú‚îÄ SubClass: {result['mapped_subclass']}")
                print(f"   ‚îú‚îÄ Class: {result['class']}")
                print(f"   ‚îú‚îÄ Text: {result['text']}")
                print(f"   ‚îî‚îÄ Confidence: {result['confidence']:.2%}\n")
            elif show_unmapped:
                print(f"{i}. ‚ö†Ô∏è  Detected: {result['detected_subclass']}")
                print(f"   ‚îî‚îÄ Kh√¥ng t√¨m th·∫•y mapping trong CSV\n")

        return results

    def process_folder(self, image_folder, output_csv='results.csv',
                    confidence_threshold=0.5):
        """
        X·ª≠ l√Ω t·∫•t c·∫£ ·∫£nh trong th∆∞ m·ª•c
        """
        print(f"\n{'='*60}")
        print(f"üìÅ X·ª≠ l√Ω th∆∞ m·ª•c: {image_folder}")
        print(f"{'='*60}\n")

        # L·∫•y t·∫•t c·∫£ ·∫£nh
        image_extensions = ['.jpg', '.jpeg', '.png', '.bmp']
        image_files = []

        for ext in image_extensions:
            image_files.extend(Path(image_folder).glob(f'*{ext}'))
            image_files.extend(Path(image_folder).glob(f'*{ext.upper()}'))

        if not image_files:
            print("‚ùå Kh√¥ng t√¨m th·∫•y ·∫£nh n√†o!")
            return None

        print(f"T√¨m th·∫•y {len(image_files)} ·∫£nh\n")

        all_results = []

        for img_path in image_files:
            results = self.predict_and_map(str(img_path), confidence_threshold)

            for result in results:
                result['image'] = os.path.basename(str(img_path))
                all_results.append(result)

            print(f"‚úì {os.path.basename(str(img_path))}: {len(results)} detections")

        # T·∫°o DataFrame
        df = pd.DataFrame(all_results)

        # L·ªçc ch·ªâ l·∫•y k·∫øt qu·∫£ c√≥ mapping
        df_mapped = df[df['mapped_subclass'].notna()].copy()

        # S·∫Øp x·∫øp
        df_mapped = df_mapped.sort_values(['image', 'confidence'],
                                        ascending=[True, False])

        # L∆∞u file
        df_mapped.to_csv(output_csv, index=False, encoding='utf-8-sig')

        print(f"\n{'='*60}")
        print(f"‚úÖ Ho√†n th√†nh!")
        print(f"üìä T·ªïng detections: {len(all_results)}")
        print(f"‚úì C√≥ mapping: {len(df_mapped)}")
        print(f"‚úó Kh√¥ng mapping: {len(all_results) - len(df_mapped)}")
        print(f"üíæ ƒê√£ l∆∞u: {output_csv}")
        print(f"{'='*60}\n")

        # Th·ªëng k√™
        if not df_mapped.empty:
            print("üìà Top 10 Class ph·ªï bi·∫øn:")
            print(df_mapped['class'].value_counts().head(10))
            print("\nüìà Top 10 SubClass ph·ªï bi·∫øn:")
            print(df_mapped['mapped_subclass'].value_counts().head(10))

        return df_mapped

    def get_info_by_subclass(self, subclass_name):
        """
        Tra c·ª©u th√¥ng tin t·ª´ SubClass
        """
        matches = self.mapping_df[
            self.mapping_df['SubClass'].str.lower() == subclass_name.lower()
        ]

        if matches.empty:
            return None

        return matches[['Text', 'Class', 'SubClass']].to_dict('records')

    def draw_detections(self, frame, detections_data):
        """
        V·∫Ω bounding box v√† th√¥ng tin l√™n frame

        Args:
            frame: Frame c·∫ßn v·∫Ω
            detections_data: Danh s√°ch detection v·ªõi boxes v√† th√¥ng tin

        Returns:
            frame ƒë√£ ƒë∆∞·ª£c v·∫Ω
        """
        annotated_frame = frame.copy()

        for det in detections_data:
            # L·∫•y th√¥ng tin bounding box
            box = det['box']  # [x1, y1, x2, y2]
            confidence = det['confidence']
            label = det['label']

            # Chuy·ªÉn ƒë·ªïi t·ªça ƒë·ªô
            x1, y1, x2, y2 = map(int, box)

            # M√†u s·∫Øc (xanh l√° cho mapped, v√†ng cho unmapped)
            color = (0, 255, 0) if det.get('mapped', False) else (0, 255, 255)

            # V·∫Ω bounding box
            cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), color, 2)

            # Chu·∫©n b·ªã text
            text = f"{label} {confidence:.2f}"

            # T√≠nh k√≠ch th∆∞·ªõc text
            font = cv2.FONT_HERSHEY_SIMPLEX
            font_scale = 0.6
            thickness = 2
            (text_width, text_height), baseline = cv2.getTextSize(
                text, font, font_scale, thickness
            )

            # V·∫Ω background cho text
            cv2.rectangle(
                annotated_frame,
                (x1, y1 - text_height - 10),
                (x1 + text_width, y1),
                color,
                -1
            )

            # V·∫Ω text
            cv2.putText(
                annotated_frame,
                text,
                (x1, y1 - 5),
                font,
                font_scale,
                (0, 0, 0),
                thickness
            )

            # V·∫Ω th√™m th√¥ng tin Class n·∫øu c√≥ mapping
            if det.get('mapped', False) and det.get('class_name'):
                class_text = f"{det['class_name']}"
                cv2.putText(
                    annotated_frame,
                    class_text,
                    (x1, y2 + 20),
                    font,
                    0.5,
                    color,
                    1
                )

        return annotated_frame

    def predict_and_map_with_boxes(self, frame, confidence_threshold=0.5):
        """
        Detect object, map v·ªõi CSV v√† tr·∫£ v·ªÅ c·∫£ th√¥ng tin boxes

        Args:
            frame: Frame ho·∫∑c ƒë∆∞·ªùng d·∫´n ·∫£nh
            confidence_threshold: Ng∆∞·ª°ng confidence

        Returns:
            List c√°c detection v·ªõi boxes v√† mapping info
        """
        # YOLO Predict
        results = self.model.predict(frame, verbose=False)

        detections_data = []

        for result in results:
            boxes = result.boxes

            if boxes is not None:
                for box in boxes:
                    confidence = float(box.conf)

                    if confidence >= confidence_threshold:
                        class_id = int(box.cls)
                        class_name = result.names[class_id]

                        # L·∫•y t·ªça ƒë·ªô bounding box
                        xyxy = box.xyxy[0].cpu().numpy()  # [x1, y1, x2, y2]

                        # Map v·ªõi CSV
                        matches = self.mapping_df[
                            self.mapping_df['SubClass'].str.lower() == class_name.lower()
                        ]

                        if not matches.empty:
                            row = matches.iloc[0]
                            detection = {
                                'box': xyxy,
                                'confidence': confidence,
                                'label': row['SubClass'],
                                'class_name': row['Class'],
                                'text': row['Text'],
                                'detected_subclass': class_name,
                                'mapped': True
                            }
                        else:
                            detection = {
                                'box': xyxy,
                                'confidence': confidence,
                                'label': class_name,
                                'class_name': None,
                                'text': None,
                                'detected_subclass': class_name,
                                'mapped': False
                            }

                        detections_data.append(detection)

        return detections_data

    def process_video_with_output(self, video_path, output_path=None,
                                top_k=5, top_n_classes=3, confidence_threshold=0.5,
                                fps_detect=1, max_duration=15,
                                output_fps=None, save_frames=False,
                                output_folder='video_frames'):
        """
        X·ª≠ l√Ω video, l∆∞u video k·∫øt qu·∫£ v·ªõi bounding box v√† t·ªïng h·ª£p th·ªëng k√™

        Args:
            video_path: ƒê∆∞·ªùng d·∫´n video input
            output_path: ƒê∆∞·ªùng d·∫´n video output (None = t·ª± ƒë·ªông t·∫°o)
            top_k: S·ªë l∆∞·ª£ng top objects (SubClass)
            top_n_classes: S·ªë l∆∞·ª£ng top Classes c√≥ nhi·ªÅu object nh·∫•t
            confidence_threshold: Ng∆∞·ª°ng confidence
            fps_detect: S·ªë frame/gi√¢y ƒë·ªÉ ch·∫°y detection (1 = detect m·ªói gi√¢y)
            max_duration: Th·ªùi l∆∞·ª£ng t·ªëi ƒëa x·ª≠ l√Ω (gi√¢y)
            output_fps: FPS c·ªßa video output (None = gi·ªØ nguy√™n FPS g·ªëc)
            save_frames: C√≥ l∆∞u frames ƒë√£ extract kh√¥ng
            output_folder: Th∆∞ m·ª•c l∆∞u frames

        Returns:
            dict: K·∫øt qu·∫£ t·ªïng h·ª£p
        """
        print(f"\n{'='*70}")
        print(f"üé• B·∫ÆT ƒê·∫¶U X·ª¨ L√ù VIDEO V·ªöI L∆ØU K·∫æT QU·∫¢")
        print(f"{'='*70}")
        print(f"üìπ Video: {os.path.basename(video_path)}")

        # ========== M·ªû VIDEO ==========
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            print("‚ùå Kh√¥ng th·ªÉ m·ªü video!")
            return None

        # L·∫•y th√¥ng tin video
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        video_fps = cap.get(cv2.CAP_PROP_FPS)
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        duration = total_frames / video_fps

        print(f"\nüìä Th√¥ng tin video:")
        print(f"‚îú‚îÄ K√≠ch th∆∞·ªõc: {width}x{height}")
        print(f"‚îú‚îÄ FPS g·ªëc: {video_fps:.2f}")
        print(f"‚îú‚îÄ T·ªïng frames: {total_frames}")
        print(f"‚îî‚îÄ Th·ªùi l∆∞·ª£ng: {duration:.2f} gi√¢y")


        if output_fps is None:
            output_fps = video_fps

        # T·∫°o VideoWriter
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')

        if output_path is not None:
            out = cv2.VideoWriter(output_path, fourcc, output_fps, (width, height))

        print(f"\nüìπ Video output:")
        print(f"‚îú‚îÄ ƒê∆∞·ªùng d·∫´n: {output_path}")
        print(f"‚îú‚îÄ FPS: {output_fps:.2f}")
        print(f"‚îî‚îÄ Detect rate: {fps_detect} frame/gi√¢y")

        # T√≠nh to√°n
        process_duration = min(duration, max_duration)
        frame_interval = int(video_fps / fps_detect)
        total_process_frames = int(process_duration * video_fps)

        print(f"\n‚öôÔ∏è C·∫•u h√¨nh x·ª≠ l√Ω:")
        print(f"‚îú‚îÄ X·ª≠ l√Ω {process_duration:.2f}s / {duration:.2f}s")
        print(f"‚îú‚îÄ T·ªïng frames s·∫Ω x·ª≠ l√Ω: {total_process_frames}")
        print(f"‚îî‚îÄ Detection m·ªói {frame_interval} frames")

        # T·∫°o th∆∞ m·ª•c l∆∞u frames n·∫øu c·∫ßn
        if save_frames:
            os.makedirs(output_folder, exist_ok=True)

        # ========== X·ª¨ L√ù VIDEO ==========
        print(f"\n{'‚îÄ'*60}")
        print("üîÑ B·∫ÆT ƒê·∫¶U X·ª¨ L√ù")
        print(f"{'‚îÄ'*60}\n")

        all_detections = []
        frame_results = []
        current_detections = []  # L∆∞u detection hi·ªán t·∫°i ƒë·ªÉ √°p d·ª•ng cho frames gi·ªØa

        frame_count = 0
        detect_count = 0

        while frame_count < total_process_frames:
            ret, frame = cap.read()
            if not ret:
                break

            # Ki·ªÉm tra xem c√≥ c·∫ßn ch·∫°y detection kh√¥ng
            should_detect = (frame_count % frame_interval == 0)

            if should_detect:
                # Ch·∫°y detection
                current_detections = self.predict_and_map_with_boxes(
                    frame, confidence_threshold
                )
                detect_count += 1

                # L∆∞u k·∫øt qu·∫£
                time_stamp = frame_count / video_fps
                frame_results.append({
                    'frame': frame_count,
                    'time': time_stamp,
                    'detections': current_detections,
                    'count': len(current_detections)
                })

                # Th√™m v√†o all_detections
                all_detections.extend(current_detections)

                # In progress
                if detect_count % 5 == 0 or detect_count == 1:
                    print(f"‚è≥ ƒê√£ detect {detect_count} frames - "
                        f"T√¨m th·∫•y {len(current_detections)} objects t·∫°i {time_stamp:.1f}s")

                # L∆∞u frame n·∫øu c·∫ßn
                if save_frames:
                    frame_filename = f"frame_{detect_count:04d}_at_{time_stamp:.2f}s.jpg"
                    frame_path = os.path.join(output_folder, frame_filename)
                    cv2.imwrite(frame_path, frame)

            # V·∫Ω bounding box (s·ª≠ d·ª•ng detection g·∫ßn nh·∫•t)
            annotated_frame = self.draw_detections(frame, current_detections)

            # Th√™m th√¥ng tin timestamp
            timestamp_text = f"Time: {frame_count/video_fps:.2f}s | Objects: {len(current_detections)}"
            cv2.putText(
                annotated_frame,
                timestamp_text,
                (10, 30),
                cv2.FONT_HERSHEY_SIMPLEX,
                0.7,
                (255, 255, 255),
                2
            )

            # Ghi frame v√†o video output
            if output_path is not None:
                out.write(annotated_frame)

            frame_count += 1

        # ƒê√≥ng video
        cap.release()
        if output_path is not None:
            out.release()

        print(f"\n‚úÖ ƒê√£ x·ª≠ l√Ω {frame_count} frames")
        print(f"‚úÖ ƒê√£ ch·∫°y detection tr√™n {detect_count} frames")
        print(f"üíæ Video ƒë√£ l∆∞u: {output_path}")

        # ========== T·ªîNG H·ª¢P TH·ªêNG K√ä ==========
        print(f"\n{'‚îÄ'*60}")
        print("üìä T·ªîNG H·ª¢P K·∫æT QU·∫¢")
        print(f"{'‚îÄ'*60}")

        if all_detections:
            # ƒê·∫øm c√°c detection (ch·ªâ nh·ªØng c√°i ƒë√£ map)
            mapped_detections = [d for d in all_detections if d['mapped']]

            if mapped_detections:
                subclass_counter = Counter()
                class_counter = Counter()
                class_object_counter = Counter()  # ƒê·∫øm t·ªïng s·ªë object cho m·ªói Class
                confidence_dict = {}
                class_confidence_dict = {}  # L∆∞u confidence cho m·ªói Class

                for det in mapped_detections:
                    subclass = det['label']
                    main_class = det['class_name']

                    subclass_counter[subclass] += 1
                    class_counter[main_class] += 1
                    class_object_counter[main_class] += 1  # ƒê·∫øm m·ªói object thu·ªôc Class

                    # L∆∞u confidence cho SubClass
                    if subclass not in confidence_dict:
                        confidence_dict[subclass] = []
                    confidence_dict[subclass].append(det['confidence'])

                    # L∆∞u confidence cho Class
                    if main_class not in class_confidence_dict:
                        class_confidence_dict[main_class] = []
                    class_confidence_dict[main_class].append(det['confidence'])

                # T√≠nh confidence trung b√¨nh cho SubClass
                avg_confidence = {k: np.mean(v) for k, v in confidence_dict.items()}

                # T√≠nh confidence trung b√¨nh cho Class
                class_avg_confidence = {k: np.mean(v) for k, v in class_confidence_dict.items()}

                print(f"\nüìà Th·ªëng k√™ t·ªïng quan:")
                print(f"‚îú‚îÄ T·ªïng detections: {len(all_detections)}")
                print(f"‚îú‚îÄ ƒê√£ mapping: {len(mapped_detections)}")
                print(f"‚îú‚îÄ SubClass unique: {len(subclass_counter)}")
                print(f"‚îî‚îÄ Class unique: {len(class_counter)}")

                # ========== TOP K OBJECTS (SUBCLASS) ==========
                print(f"\n{'‚îÄ'*60}")
                print(f"üèÜ TOP {top_k} OBJECTS (SUBCLASS) XU·∫§T HI·ªÜN NHI·ªÄU NH·∫§T")
                print(f"{'‚îÄ'*60}")

                top_results = []
                for i, (subclass, count) in enumerate(subclass_counter.most_common(top_k), 1):
                    info = self.mapping_df[
                        self.mapping_df['SubClass'] == subclass
                    ].iloc[0]

                    frequency = (count / detect_count) * 100

                    result_item = {
                        'rank': i,
                        'subclass': subclass,
                        'class': info['Class'],
                        'text': info['Text'],
                        'appearances': count,
                        'total_detections': detect_count,
                        'frequency': frequency,
                        'avg_confidence': avg_confidence[subclass]
                    }

                    top_results.append(result_item)

                    print(f"\nü•á TOP {i}: {subclass}")
                    print(f"   ‚îú‚îÄ Class: {info['Class']}")
                    print(f"   ‚îú‚îÄ Xu·∫•t hi·ªán: {count}/{detect_count} frames ({frequency:.1f}%)")
                    print(f"   ‚îî‚îÄ Confidence TB: {avg_confidence[subclass]:.2%}")

                # ========== TOP N CLASSES C√ì NHI·ªÄU OBJECT NH·∫§T ==========
                print(f"\n{'‚îÄ'*60}")
                print(f"üèÜ TOP {top_n_classes} CLASSES C√ì NHI·ªÄU OBJECT NH·∫§T")
                print(f"{'‚îÄ'*60}")

                top_classes = []
                for i, (main_class, total_objects) in enumerate(class_object_counter.most_common(top_n_classes), 1):
                    # T√¨m c√°c SubClass thu·ªôc Class n√†y
                    subclasses_in_class = []
                    for subclass, count in subclass_counter.items():
                        info = self.mapping_df[
                            self.mapping_df['SubClass'] == subclass
                        ].iloc[0]
                        if info['Class'] == main_class:
                            subclasses_in_class.append({
                                'subclass': subclass,
                                'count': count
                            })

                    # S·∫Øp x·∫øp SubClass theo s·ªë l∆∞·ª£ng
                    subclasses_in_class.sort(key=lambda x: x['count'], reverse=True)

                    class_frequency = (total_objects / len(mapped_detections)) * 100

                    class_item = {
                        'rank': i,
                        'class': main_class,
                        'total_objects': total_objects,
                        'unique_subclasses': len(subclasses_in_class),
                        'subclasses': subclasses_in_class,
                        'frequency': class_frequency,
                        'avg_confidence': class_avg_confidence[main_class]
                    }

                    top_classes.append(class_item)

                    print(f"\nüèÖ TOP {i}: {main_class}")
                    print(f"   ‚îú‚îÄ T·ªïng s·ªë objects: {total_objects}")
                    print(f"   ‚îú‚îÄ T·ª∑ l·ªá: {class_frequency:.1f}% trong t·ªïng s·ªë detections")
                    print(f"   ‚îú‚îÄ S·ªë SubClass kh√°c nhau: {len(subclasses_in_class)}")
                    print(f"   ‚îú‚îÄ Confidence TB: {class_avg_confidence[main_class]:.2%}")
                    print(f"   ‚îî‚îÄ Chi ti·∫øt SubClass:")

                    # Hi·ªÉn th·ªã top 3 SubClass c·ªßa Class n√†y
                    for j, sub_item in enumerate(subclasses_in_class[:3], 1):
                        print(f"       {j}. {sub_item['subclass']}: {sub_item['count']} objects")

                    if len(subclasses_in_class) > 3:
                        print(f"       ... v√† {len(subclasses_in_class) - 3} SubClass kh√°c")

                # ========== TH·ªêNG K√ä THEO TH·ªúI GIAN ==========
                print(f"\n{'‚îÄ'*60}")
                print("üìà PH√ÇN B·ªê THEO TH·ªúI GIAN")
                print(f"{'‚îÄ'*60}")

                # Chia video th√†nh c√°c kho·∫£ng th·ªùi gian
                time_segments = 5  # Chia th√†nh 5 ph·∫ßn
                segment_duration = process_duration / time_segments

                time_distribution = []
                for seg in range(time_segments):
                    start_time = seg * segment_duration
                    end_time = (seg + 1) * segment_duration

                    # ƒê·∫øm detections trong kho·∫£ng th·ªùi gian n√†y
                    segment_detections = 0
                    for frame_result in frame_results:
                        if start_time <= frame_result['time'] < end_time:
                            segment_detections += frame_result['count']

                    time_distribution.append({
                        'segment': seg + 1,
                        'start': start_time,
                        'end': end_time,
                        'detections': segment_detections
                    })

                    print(f"   Ph√∫t {start_time:.1f}-{end_time:.1f}s: {segment_detections} objects")

                # T·∫°o summary ƒë·∫ßy ƒë·ªß
                summary = {
                    'video_info': {
                        'input_path': video_path,
                        'output_path': output_path,
                        'duration': duration,
                        'processed_duration': process_duration,
                        'total_frames': frame_count,
                        'detected_frames': detect_count,
                        'resolution': f"{width}x{height}",
                        'fps': video_fps,
                        'output_fps': output_fps
                    },
                    'detection_summary': {
                        'total_detections': len(all_detections),
                        'mapped_detections': len(mapped_detections),
                        'unique_subclasses': len(subclass_counter),
                        'unique_classes': len(class_counter)
                    },
                    'top_objects': top_results,  # Top k objects (SubClass)
                    'top_classes': top_classes,  # Top n Classes c√≥ nhi·ªÅu object nh·∫•t
                    'time_distribution': time_distribution,
                    'frame_details': frame_results,
                    'yolo_detections': all_detections

                }

                print(f"\n{'='*70}")
                print("‚úÖ HO√ÄN TH√ÄNH!")
                print(f"{'='*70}")

                return summary

        print("\n‚ùå Kh√¥ng ph√°t hi·ªán object n√†o!")
        return None
    
    def process_video(self, video_path, confidence_threshold=0.5, fps_detect=1):
        """X·ª≠ l√Ω video v√† tr·∫£ v·ªÅ list ObjectDetection (D√πng cho Bayesian Classifier)"""
        cap = cv2.VideoCapture(video_path)
        video_fps = cap.get(cv2.CAP_PROP_FPS)
        frame_interval = int(video_fps / fps_detect)
        
        all_objects = []
        frame_count = 0

        while True:
            ret, frame = cap.read()
            if not ret: break

            if frame_count % frame_interval == 0:
                raw_dets = self.predict_and_map_with_boxes(frame, confidence_threshold)
                
                # Group by subclass trong frame hi·ªán t·∫°i
                subclass_groups = {}
                for d in raw_dets:
                    # Ch·ªâ l·∫•y detection c√≥ mapping h·ª£p l·ªá
                    if d.get('mapped'):
                        lbl = d['label']
                        if lbl not in subclass_groups:
                            subclass_groups[lbl] = {'confs': [], 'boxes': []}
                        subclass_groups[lbl]['confs'].append(d['confidence'])
                        subclass_groups[lbl]['boxes'].append(d['box'])

                time_stamp = frame_count / video_fps
                
                for sub, data in subclass_groups.items():
                    obj = ObjectDetection(
                        subclass=sub,
                        confidence=np.mean(data['confs']),
                        frame_id=frame_count,
                        time_stamp=time_stamp,
                        count=len(data['boxes']),
                        bboxs=data['boxes']
                    )
                    all_objects.append(obj)
            
            frame_count += 1
        
        cap.release()
        return all_objects


# ƒê·ªãnh nghƒ©a c·∫•u tr√∫c d·ªØ li·ªáu
class ObjectDetection:
    def __init__(self, subclass, confidence, frame_id, time_stamp, count, bboxs):
        self.subclass = subclass          # e.g., "binh_bong_dua"
        self.confidence = confidence      # trung b√¨nh c√°c confidence trong frame
        self.frame_id = frame_id          # s·ªë th·ª© t·ª± frame
        self.time_stamp = time_stamp      # th·ªùi gian (gi√¢y)
        self.count = count        # s·ªë l·∫ßn subclass xu·∫•t hi·ªán trong frame
        self.bboxs = bboxs                # danh s√°ch bounding box (list of [x1, y1, x2, y2])
    def __repr__(self):
        return (f"ObjectDetection(subclass='{self.subclass}', "
                f"confidence={self.confidence:.2f}, "
                f"frame_id={self.frame_id}, "
                f"time_stamp={self.time_stamp:.2f}, "
                f"count={self.count}, "
                f"bboxs={self.bboxs})")

# Database r√†ng bu·ªôc: dict[l·ªÖ_h·ªôi] = list[r√†ng_bu·ªôc]
# M·ªói r√†ng_bu·ªôc l√† tuple (type, params, is_hard, weight, threshold)
# V√≠ d·ª•: ("is_presence", ["binh_bong_dua", "hoa_sen"], True, 1.0, None)  # Hard, ph·∫£i c√≥ c·∫£ hai
# ("at_least", ["binh_bong_dua"], True, 1.0, 10)  # Hard, √≠t nh·∫•t 10 instances
# ("is_on", ["bong_dua", "trai_dua"], False, 0.5, None)  # Soft, weight 0.5 n·∫øu "bong_dua" on "trai_dua" (c√≥ th·ªÉ d√πng spatial check)
# ("confidence_min", ["all"], True, 1.0, 0.7)  # Hard, avg confidence >=0.7

# ==========================================
# C·∫§U H√åNH TO√ÄN C·ª§C (T·ª´ PSEUDO)
# ==========================================
GLOBAL_CONFIG = {
    "T_high": 0.85,    # Ng∆∞·ª°ng tin c·∫≠y cao ƒë·ªÉ ch·ªçn ·ª©ng vi√™n ngay
    "T_low": 0.50,     # Ng∆∞·ª°ng th·∫•p nh·∫•t ƒë·ªÉ xem x√©t
    "delta": 0.25,     # Ch√™nh l·ªách t·ªëi ƒëa cho ph√©p so v·ªõi conf_max
    "T_out": 0.85      # Ng∆∞·ª°ng quy·∫øt ƒë·ªãnh cu·ªëi c√πng (sau khi h·ªèi user)
}

UNCERTAINTY_RULES = {
    "ch·∫Øc c√≥": 0.85,
    "c√≥": 1.0,
    "h√¨nh nh∆∞ c√≥": 0.6,
    "c√≥ l·∫Ω c√≥": 0.55,
    "ch·∫Øc kh√¥ng": 0.35,
    "kh√¥ng": 0.0,
    "h√¨nh nh∆∞ kh√¥ng": 0.45,
    "c√≥ l·∫Ω kh√¥ng": 0.4
}

# ==========================================
# C√ÅC H√ÄM TO√ÅN H·ªåC B·ªî TR·ª¢
# ==========================================
def clip_confidence(p):
    """Gi·ªõi h·∫°n p trong kho·∫£ng (epsilon, 1-epsilon) ƒë·ªÉ tr√°nh log(0)"""
    eps = 1e-6
    if p < eps: p = eps
    if p > 1 - eps: p = 1 - eps
    return p

def logit(p):
    """Chuy·ªÉn ƒë·ªïi x√°c su·∫•t p sang kh√¥ng gian Logit (Log-odds)"""
    p = clip_confidence(p)
    return math.log(p / (1 - p))

def sigmoid(x):
    """Chuy·ªÉn ƒë·ªïi ng∆∞·ª£c t·ª´ Logit sang x√°c su·∫•t [0, 1]"""
    return 1 / (1 + math.exp(-x))

# ==========================================
# CLASS DATA STRUCTURE
# ==========================================
class ObjectDetection:
    def __init__(self, subclass, confidence, frame_id, time_stamp, count, bboxs):
        self.subclass = subclass          # e.g., "binh_bong_dua"
        self.confidence = confidence      # trung b√¨nh c√°c confidence trong frame
        self.frame_id = frame_id          # s·ªë th·ª© t·ª± frame
        self.time_stamp = time_stamp      # th·ªùi gian (gi√¢y)
        self.count = count                # s·ªë l·∫ßn subclass xu·∫•t hi·ªán trong frame
        self.bboxs = bboxs                # danh s√°ch bounding box [x1, y1, x2, y2]
    
    def __repr__(self):
        return f"<Obj: {self.subclass}, Conf: {self.confidence:.2f}, Count: {self.count}>"


# ==========================================
# PH·∫¶N 2: BAYESIAN REASONING CORE (LOGIT + ADDITIVE ONLY)
# ==========================================
class BayesianFestivalClassifier:
    def __init__(self, api_key):
        self.api_key = api_key
        # S·ª≠ d·ª•ng model m·∫°nh h∆°n m·ªôt ch√∫t ƒë·ªÉ parse JSON t·ªët h∆°n n·∫øu c·∫ßn, ho·∫∑c flash v·∫´n ok
        self.llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", google_api_key=api_key, max_retries=3, temperature=0)

    def _index_detections(self, detections):
        by_subclass = defaultdict(list)
        by_frame = defaultdict(list)
        for d in detections:
            by_subclass[d.subclass].append(d)
            by_frame[d.frame_id].append(d)
        return by_subclass, by_frame

    def _check_is_on(self, top_sub, bot_sub, by_subclass, by_frame):
        relevant_frames = set(d.frame_id for d in by_subclass[top_sub]) & set(d.frame_id for d in by_subclass[bot_sub])
        for fid in relevant_frames:
            tops = [d for d in by_frame[fid] if d.subclass == top_sub]
            bots = [d for d in by_frame[fid] if d.subclass == bot_sub]
            for t in tops:
                for b in bots:
                    for box_t in t.bboxs:
                        for box_b in b.bboxs:
                            x_overlap = max(0, min(box_t[2], box_b[2]) - max(box_t[0], box_b[0]))
                            width_t = box_t[2] - box_t[0]
                            vertical_gap = box_b[1] - box_t[3]
                            if width_t > 0 and (x_overlap/width_t) > 0.3 and -50 <= vertical_gap <= 50:
                                return True
        return False

    def check_constraints(self, rule, by_subclass, by_frame):
        ctype, params, is_hard, weight, threshold = rule
        satisfied = False
        if ctype == "is_presence":
            satisfied = len([p for p in params if p not in by_subclass]) == 0
        elif ctype == "is_presence_in_frame":
            for fid, dets in by_frame.items():
                subs = {d.subclass for d in dets}
                if all(p in subs for p in params):
                    satisfied = True; break
        elif ctype == "at_least":
            total = sum(sum(d.count for d in by_subclass[p]) for p in params if p in by_subclass)
            satisfied = total >= (threshold or 1)
        elif ctype == "at_least_in_frame":
            for fid, dets in by_frame.items():
                cnt = sum(d.count for d in dets if d.subclass in params)
                if cnt >= (threshold or 1):
                    satisfied = True; break
        elif ctype == "confidence_min":
            target = list(by_subclass.keys()) if "all" in params else [p for p in params if p in by_subclass]
            if target:
                avg = sum(d.confidence * d.count for s in target for d in by_subclass[s]) / sum(d.count for s in target for d in by_subclass[s])
                satisfied = avg >= (threshold or 0)
        elif ctype == "is_on" and len(params) == 2:
            satisfied = self._check_is_on(params[0], params[1], by_subclass, by_frame)
        return satisfied

    def calculate_initial_logits(self, detections):
        by_subclass, by_frame = self._index_detections(detections)
        festival_logits = {}
        festival_unsatisfied = defaultdict(list)

        for festival, rules in CONSTRAINTS_DB.items():
            current_logit = 0.0
            for rule in rules:
                is_satisfied = self.check_constraints(rule, by_subclass, by_frame)
                weight = rule[3]
                if is_satisfied:
                    current_logit += weight
                else:
                    festival_unsatisfied[festival].append(rule)
            festival_logits[festival] = current_logit
        return festival_logits, festival_unsatisfied

    def select_candidates(self, festival_logits):
        festival_probs = {f: sigmoid(l) for f, l in festival_logits.items()}
        if not festival_probs: return []
        max_prob = max(festival_probs.values())
        candidates = []
        
        print(f"\nB·∫¢NG X·∫æP H·∫†NG BAN ƒê·∫¶U:")
        for f, p in sorted(festival_probs.items(), key=lambda x: x[1], reverse=True):
            print(f"   - {f}: {p:.2%} (Logit: {festival_logits[f]:.2f})")

        for f, p in festival_probs.items():
            if p >= GLOBAL_CONFIG["T_high"]: candidates.append(f)
            elif p >= GLOBAL_CONFIG["T_low"] and (max_prob - p) <= GLOBAL_CONFIG["delta"]: candidates.append(f)
        return candidates

    # ==========================================
    # PH·∫¶N 3: LLM INTERACTION - CONSOLIDATED QUESTION
    # ==========================================
    
    def generate_consolidated_question(self, candidates, festival_unsatisfied):
        """
        T·∫°o 1 c√¢u h·ªèi duy nh·∫•t t·ªïng h·ª£p t·∫•t c·∫£ c√°c ƒë·∫∑c tr∆∞ng c√≤n thi·∫øu.
        """
        all_missing_features = set()
        for fest in candidates:
            rules = festival_unsatisfied[fest]
            for rule in rules:
                # Rule[1] l√† params (list c√°c subclass c·∫ßn t√¨m)
                all_missing_features.update(rule[1])
        
        if not all_missing_features:
            return None
        
        feature_list_str = ", ".join(all_missing_features)
        candidate_str = ", ".join(candidates)
        
        question = (
            f"H·ªá th·ªëng ƒëang ph√¢n v√¢n gi·ªØa c√°c l·ªÖ h·ªôi: {candidate_str}. "
            f"B·∫°n h√£y quan s√°t k·ªπ video v√† cho bi·∫øt b·∫°n c√≥ th·∫•y c√°c ƒë·∫∑c tr∆∞ng sau kh√¥ng: "
            f"{feature_list_str}?"
        )
        
        # Tr·∫£ v·ªÅ c·∫£ text c√¢u h·ªèi v√† list features ƒë·ªÉ d√πng cho b∆∞·ªõc analyze sau n√†y
        return {
            "question_text": question,
            "target_features": list(all_missing_features)
        }

    def analyze_complex_answer(self, question, user_answer, target_features):
        """
        Ph√¢n t√≠ch c√¢u tr·∫£ l·ªùi ph·ª©c t·∫°p b·∫±ng LLM v√† map v·ªõi UNCERTAINTY_RULES.
        Tr·∫£ v·ªÅ JSON mapping: {feature: {"status": True/False, "confidence": float}}
        """
        # Chuy·ªÉn rules th√†nh string ƒë·ªÉ ƒë∆∞a v√†o prompt
        rules_desc = json.dumps(UNCERTAINTY_RULES, ensure_ascii=False)
        features_desc = ", ".join(target_features)
        
        prompt = f"""
        Nhi·ªám v·ª•: Ph√¢n t√≠ch c√¢u tr·∫£ l·ªùi c·ªßa ng∆∞·ªùi d√πng v·ªÅ s·ª± xu·∫•t hi·ªán c·ªßa c√°c v·∫≠t th·ªÉ trong video.
        
        Danh s√°ch v·∫≠t th·ªÉ c·∫ßn t√¨m (Features): {features_desc}
        
        B·∫£ng ƒëi·ªÉm tin c·∫≠y (Uncertainty Rules):
        {rules_desc}
        
        C√¢u h·ªèi c·ªßa h·ªá th·ªëng: "{question}"
        C√¢u tr·∫£ l·ªùi c·ªßa ng∆∞·ªùi d√πng: "{user_answer}"
        
        Y√™u c·∫ßu Output:
        Tr·∫£ v·ªÅ m·ªôt JSON object duy nh·∫•t. Key l√† t√™n v·∫≠t th·ªÉ (trong danh s√°ch Features), Value l√† object ch·ª©a:
        - "status": true (n·∫øu ng∆∞·ªùi d√πng b·∫£o c√≥), false (n·∫øu ng∆∞·ªùi d√πng b·∫£o kh√¥ng).
        - "confidence": ƒêi·ªÉm s·ªë l·∫•y ch√≠nh x√°c t·ª´ B·∫£ng ƒëi·ªÉm tin c·∫≠y d·ª±a tr√™n t·ª´ ng·ªØ ng∆∞·ªùi d√πng d√πng.
        
        V√≠ d·ª•: N·∫øu user n√≥i "C√≥ ƒë√®n gi√≥ nh∆∞ng ch·∫Øc kh√¥ng c√≥ ghe ngo", output:
        {{
            "ƒë√®n gi√≥": {{"status": true, "confidence": 1.0}},
            "ghe ngo": {{"status": false, "confidence": 0.35}}
        }}
        
        Ch·ªâ tr·∫£ v·ªÅ JSON, kh√¥ng th√™m markdown.
        """
        
        parser = JsonOutputParser()
        try:
            result = self.llm.invoke(prompt).content
            # Clean markdown if exists
            if "```json" in result:
                result = result.split("```json")[1].split("```")[0]
            parsed_result = json.loads(result.strip())
            return parsed_result
        except Exception as e:
            print(f"L·ªói parse JSON t·ª´ LLM: {e}")
            return {}

    def update_logits_from_consolidated_answer(self, festival_logits, candidates, festival_unsatisfied, parsed_answer):
        """
        C·∫≠p nh·∫≠t ƒëi·ªÉm Logit d·ª±a tr√™n k·∫øt qu·∫£ ph√¢n t√≠ch JSON.
        (C√≥ th∆∞·ªüng c√≥ ph·∫°t).
        """
        final_logits = festival_logits.copy()
        
        print("\nC·∫≠p nh·∫≠t ƒëi·ªÉm d·ª±a tr√™n c√¢u tr·∫£ l·ªùi...")
        
        for fest in candidates:
            unsatisfied_rules = festival_unsatisfied[fest]
            
            for rule in unsatisfied_rules:
                params = rule[1]
                weight = rule[3]
                
                # Ki·ªÉm tra xem feature trong rule n√†y c√≥ ƒë∆∞·ª£c user nh·∫Øc t·ªõi kh√¥ng
                # M·ªôt rule c√≥ th·ªÉ y√™u c·∫ßu nhi·ªÅu params (VD: ["A", "B"]). 
                # ƒê∆°n gi·∫£n h√≥a: N·∫øu b·∫•t k·ª≥ param n√†o trong rule ƒë∆∞·ª£c nh·∫Øc t·ªõi
                
                for param in params:
                    if param in parsed_answer:
                        data = parsed_answer[param]
                        status = data.get("status")
                        conf = data.get("confidence", 0.5)
                        
                        if status is True:
                            # User x√°c nh·∫≠n C√ì -> C·ªông ƒëi·ªÉm
                            # Delta = Weight * Confidence
                            delta = weight * conf
                            final_logits[fest] += delta
                            print(f"   => [{fest}] '{param}' C√ì (conf={conf}): +{delta:.2f}")
                            
                        elif status is False:
                            # User x√°c nh·∫≠n KH√îNG -> Tr·ª´ ƒëi·ªÉm (Ph∆∞∆°ng √°n A)
                            # Penalty = (Weight * Confidence) / 2
                            penalty = (weight * conf) / 2
                            final_logits[fest] -= penalty
                            print(f"   => [{fest}] '{param}' KH√îNG (conf={conf}): -{penalty:.2f}")
                            
        return final_logits


    def decide_final_result(self, final_logits):
        """K·∫øt lu·∫≠n cu·ªëi c√πng"""
        final_probs = {f: sigmoid(l) for f, l in final_logits.items()}
        results = []
        
        print(f"\n K·∫æT QU·∫¢ CU·ªêI C√ôNG:")
        sorted_res = sorted(final_probs.items(), key=lambda x: x[1], reverse=True)
        for f, p in sorted_res:
            status = "ƒê·∫†T" if p >= GLOBAL_CONFIG["T_out"] else "TR∆Ø·ª¢T"
            print(f"   {f}: {p:.2%} ({status})")
            if p >= GLOBAL_CONFIG["T_out"]:
                results.append(f)
                
        return results, final_probs
